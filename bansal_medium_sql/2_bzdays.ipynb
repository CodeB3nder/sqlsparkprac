{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120bab30-735e-4821-b990-af4f59ca85d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+\n",
      "|ticket_id|create_date        |resolved_date      |\n",
      "+---------+-------------------+-------------------+\n",
      "|1        |2022-08-01 00:00:00|2022-08-03 00:00:00|\n",
      "|2        |2022-08-01 00:00:00|2022-08-12 00:00:00|\n",
      "|3        |2022-08-01 00:00:00|2022-08-16 00:00:00|\n",
      "+---------+-------------------+-------------------+\n",
      "\n",
      "+-------------------+----------------+\n",
      "|holiday_date       |reason          |\n",
      "+-------------------+----------------+\n",
      "|2022-08-11 00:00:00|Rakhi           |\n",
      "|2022-08-15 00:00:00|Independence day|\n",
      "+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"TicketsAndHolidays\").getOrCreate()\n",
    "\n",
    "# -----------------------\n",
    "# Create Tickets Table\n",
    "# -----------------------\n",
    "\n",
    "# Data with datetime\n",
    "tickets_data = [\n",
    "    (\"1\", datetime.strptime('2022-08-01', '%Y-%m-%d'), datetime.strptime('2022-08-03', '%Y-%m-%d')),\n",
    "    (\"2\", datetime.strptime('2022-08-01', '%Y-%m-%d'), datetime.strptime('2022-08-12', '%Y-%m-%d')),\n",
    "    (\"3\", datetime.strptime('2022-08-01', '%Y-%m-%d'), datetime.strptime('2022-08-16', '%Y-%m-%d'))\n",
    "]\n",
    "\n",
    "# Schema\n",
    "tickets_schema = StructType([\n",
    "    StructField(\"ticket_id\", StringType(), True),\n",
    "    StructField(\"create_date\", TimestampType(), True),\n",
    "    StructField(\"resolved_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "tickets = spark.createDataFrame(tickets_data, schema=tickets_schema)\n",
    "\n",
    "# Create or replace a temp view\n",
    "tickets.createOrReplaceTempView(\"tickets\")\n",
    "\n",
    "# -----------------------\n",
    "# Create Holidays Table\n",
    "# -----------------------\n",
    "\n",
    "# Data with datetime\n",
    "holidays_data = [\n",
    "    (datetime.strptime('2022-08-11', '%Y-%m-%d'), 'Rakhi'),\n",
    "    (datetime.strptime('2022-08-15', '%Y-%m-%d'), 'Independence day')\n",
    "]\n",
    "\n",
    "# Schema\n",
    "holidays_schema = StructType([\n",
    "    StructField(\"holiday_date\", TimestampType(), True),\n",
    "    StructField(\"reason\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "holidays = spark.createDataFrame(holidays_data, schema=holidays_schema)\n",
    "\n",
    "# Create or replace a temp view\n",
    "holidays.createOrReplaceTempView(\"holidays\")\n",
    "\n",
    "# -----------------------\n",
    "# Check your tables!\n",
    "# -----------------------\n",
    "\n",
    "# Sample queries\n",
    "spark.sql(\"SELECT * FROM tickets\").show(truncate=False)\n",
    "spark.sql(\"SELECT * FROM holidays\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390917a-bd6e-4343-ba72-c54154a4c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    -- datediff(day, create_date, resolved_date) - 2* datediff(week, create_date, resolved_date) as actualdays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce100dc-a170-4cd2-9f59-68454c59554e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+--------------+----------+\n",
      "|ticket_id|        create_date|      resolved_date|no_of_holidays|actualdays|\n",
      "+---------+-------------------+-------------------+--------------+----------+\n",
      "|        1|2022-08-01 00:00:00|2022-08-03 00:00:00|             0|         2|\n",
      "|        2|2022-08-01 00:00:00|2022-08-12 00:00:00|             1|         8|\n",
      "|        3|2022-08-01 00:00:00|2022-08-16 00:00:00|             2|         9|\n",
      "+---------+-------------------+-------------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select *,\n",
    "    datediff(day, create_date, resolved_date) - 2* datediff(week, create_date, resolved_date) - no_of_holidays as actualdays\n",
    "    from (\n",
    "    select ticket_id, create_date, resolved_date, count(holiday_date) as no_of_holidays\n",
    "    from tickets left join holidays on holiday_date between create_date and resolved_date\n",
    "    group by ticket_id, create_date, resolved_date)\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "527beb99-21f2-4fe9-8305-0cdb59949b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'datediff(weekofyear(CAST(t.resolved_date AS DATE)), weekofyear(CAST(t.create_date AS DATE)))' due to data type mismatch: argument 1 requires date type, however, 'weekofyear(CAST(t.resolved_date AS DATE))' is of int type. argument 2 requires date type, however, 'weekofyear(CAST(t.create_date AS DATE))' is of int type.;\n'Project [ticket_id#226, create_date#227, resolved_date#228, no_of_holidays#279L, ((datediff(cast(resolved_date#228 as date), cast(create_date#227 as date)) - (datediff(weekofyear(cast(resolved_date#228 as date)), weekofyear(cast(create_date#227 as date))) * 2)) - no_of_holidays#279L) AS actualdays#284]\n+- Aggregate [ticket_id#226, create_date#227, resolved_date#228], [ticket_id#226, create_date#227, resolved_date#228, count(holiday_date#232) AS no_of_holidays#279L]\n   +- Join LeftOuter, ((holiday_date#232 >= create_date#227) AND (holiday_date#232 <= resolved_date#228))\n      :- SubqueryAlias t\n      :  +- LogicalRDD [ticket_id#226, create_date#227, resolved_date#228], false\n      +- SubqueryAlias h\n         +- LogicalRDD [holiday_date#232, reason#233], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1601/2670729543.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatediff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resolved_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatediff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweekofyear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resolved_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweekofyear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;34m-\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no_of_holidays\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;32m/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3035\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3036\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3038\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'datediff(weekofyear(CAST(t.resolved_date AS DATE)), weekofyear(CAST(t.create_date AS DATE)))' due to data type mismatch: argument 1 requires date type, however, 'weekofyear(CAST(t.resolved_date AS DATE))' is of int type. argument 2 requires date type, however, 'weekofyear(CAST(t.create_date AS DATE))' is of int type.;\n'Project [ticket_id#226, create_date#227, resolved_date#228, no_of_holidays#279L, ((datediff(cast(resolved_date#228 as date), cast(create_date#227 as date)) - (datediff(weekofyear(cast(resolved_date#228 as date)), weekofyear(cast(create_date#227 as date))) * 2)) - no_of_holidays#279L) AS actualdays#284]\n+- Aggregate [ticket_id#226, create_date#227, resolved_date#228], [ticket_id#226, create_date#227, resolved_date#228, count(holiday_date#232) AS no_of_holidays#279L]\n   +- Join LeftOuter, ((holiday_date#232 >= create_date#227) AND (holiday_date#232 <= resolved_date#228))\n      :- SubqueryAlias t\n      :  +- LogicalRDD [ticket_id#226, create_date#227, resolved_date#228], false\n      +- SubqueryAlias h\n         +- LogicalRDD [holiday_date#232, reason#233], false\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "joined_df = (\n",
    "    tickets.alias(\"t\")\n",
    "    .join(\n",
    "        holidays.alias(\"h\"),\n",
    "        (F.col(\"h.holiday_date\").between(F.col(\"t.create_date\"), F.col(\"t.resolved_date\"))),\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Aggregate to count number of holidays\n",
    "aggregated_df = (\n",
    "    joined_df\n",
    "    .groupBy(\"ticket_id\", \"create_date\", \"resolved_date\")\n",
    "    .agg(F.count(\"h.holiday_date\").alias(\"no_of_holidays\"))\n",
    ")\n",
    "\n",
    "# Calculate actualdays\n",
    "result_df = (\n",
    "    aggregated_df\n",
    "    .withColumn(\n",
    "        \"actualdays\",\n",
    "        F.datediff(F.col(\"resolved_date\"), F.col(\"create_date\"))\n",
    "        - 2 * F.datediff(F.weekofyear(F.col(\"resolved_date\")), F.weekofyear(F.col(\"create_date\")))\n",
    "        - F.col(\"no_of_holidays\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc878b-5ac9-4b58-8b9a-58276d6477fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

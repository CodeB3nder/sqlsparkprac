{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a10cd27-dd81-4460-b82f-c1913f50ed76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySparkTables\").getOrCreate()\n",
    "\n",
    "# Raw string data\n",
    "records_data = [\n",
    "    (\"2021-01-01\", \"A1\", \"U1\"),\n",
    "    (\"2021-01-01\", \"A1\", \"U2\"),\n",
    "    (\"2021-01-06\", \"A1\", \"U3\"),\n",
    "    (\"2021-01-02\", \"A1\", \"U1\"),\n",
    "    (\"2020-12-24\", \"A1\", \"U2\"),\n",
    "    (\"2020-12-08\", \"A1\", \"U1\"),\n",
    "    (\"2020-12-09\", \"A1\", \"U1\"),\n",
    "    (\"2021-01-10\", \"A2\", \"U4\"),\n",
    "    (\"2021-01-11\", \"A2\", \"U4\"),\n",
    "    (\"2021-01-12\", \"A2\", \"U4\"),\n",
    "    (\"2021-01-15\", \"A2\", \"U5\"),\n",
    "    (\"2020-12-17\", \"A2\", \"U4\"),\n",
    "    (\"2020-12-25\", \"A3\", \"U6\"),\n",
    "    (\"2020-12-25\", \"A3\", \"U6\"),\n",
    "    (\"2020-12-25\", \"A3\", \"U6\"),\n",
    "    (\"2020-12-06\", \"A3\", \"U7\"),\n",
    "    (\"2020-12-06\", \"A3\", \"U6\"),\n",
    "    (\"2021-01-14\", \"A3\", \"U6\"),\n",
    "    (\"2021-02-07\", \"A1\", \"U1\"),\n",
    "    (\"2021-02-10\", \"A1\", \"U2\"),\n",
    "    (\"2021-02-01\", \"A2\", \"U4\"),\n",
    "    (\"2021-02-01\", \"A2\", \"U5\"),\n",
    "    (\"2020-12-05\", \"A1\", \"U8\"),\n",
    "]\n",
    "\n",
    "# Convert string to datetime.date\n",
    "records_data_typed = [\n",
    "    (datetime.strptime(date_str, \"%Y-%m-%d\").date(), account_id, user_id)\n",
    "    for date_str, account_id, user_id in records_data\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"record_date\", DateType(), False),\n",
    "    StructField(\"account_id\", StringType(), False),\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "records_df = spark.createDataFrame(records_data_typed, schema)\n",
    "\n",
    "# Register as temporary view\n",
    "records_df.createOrReplaceTempView(\"account_records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fffb6ef1-1736-4325-bae5-2929cbe21dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+\n",
      "|user_id|  grp_date|count(1)|\n",
      "+-------+----------+--------+\n",
      "|     U4|2021-01-09|       3|\n",
      "+-------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    with ranked as (\n",
    "    select *, \n",
    "    row_number() over(partition by user_id order by record_date) rn\n",
    "    from account_records),\n",
    "    \n",
    "    grp as (select user_id, record_date,\n",
    "    DATE_SUB(record_date, rn-1) as grp_date\n",
    "    from ranked)\n",
    "    \n",
    "    select user_id, grp_date, count(*) from grp group by user_id, grp_date having count(*) = 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "728ca6a7-43ba-4aad-8394-58aff9699926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|     U4|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH consecutive_days AS\n",
    "  (SELECT user_id,\n",
    "          record_date,\n",
    "          LAG(record_date, 1) OVER (PARTITION BY user_id\n",
    "                                    ORDER BY record_date) AS prev_day,\n",
    "          LEAD(record_date, 1) OVER (PARTITION BY user_id\n",
    "                                     ORDER BY record_date) AS next_day\n",
    "   FROM account_records)\n",
    "SELECT DISTINCT user_id\n",
    "FROM consecutive_days\n",
    "WHERE DATEDIFF(record_date, prev_day) = 1\n",
    "  AND DATEDIFF(next_day, record_date) = 1;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb00ed6e-aecc-4551-adbf-8091da53eeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+\n",
      "|user_id|  grp_date|cnt|\n",
      "+-------+----------+---+\n",
      "|     U4|2021-01-09|  3|\n",
      "+-------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"record_date\")\n",
    "ranked_df = records_df.withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "grp_df = ranked_df.withColumn(\n",
    "    \"grp_date\",\n",
    "    F.date_sub(F.col(\"record_date\"), F.col(\"rn\") - F.lit(1))\n",
    ")\n",
    "result_df = grp_df.groupBy(\"user_id\", \"grp_date\") \\\n",
    "    .agg(F.count(\"*\").alias(\"cnt\")) \\\n",
    "    .filter(F.col(\"cnt\") == 3) \\\n",
    "    .select(\"user_id\", \"grp_date\", \"cnt\")\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f82a5-2699-4320-9d07-73e331d74ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

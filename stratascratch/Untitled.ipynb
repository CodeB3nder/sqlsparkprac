{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffca68c9-303a-463a-8ee9-a849e08ebb29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------------+----------------------+\n",
      "|  amt|signup_id|transaction_id|transaction_start_date|\n",
      "+-----+---------+--------------+----------------------+\n",
      "|100.0|      101|          2001|            2023-04-10|\n",
      "|250.0|      102|          2002|            2023-05-15|\n",
      "|300.0|      103|          2003|            2023-06-12|\n",
      "|150.0|      101|          2004|            2023-05-22|\n",
      "|175.0|      104|          2005|            2023-03-28|\n",
      "|220.0|      105|          2006|            2023-04-02|\n",
      "|400.0|      106|          2007|            2023-01-30|\n",
      "+-----+---------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, month\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"TransactionFilter\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"amt\", DoubleType(), True),\n",
    "    StructField(\"signup_id\", LongType(), True),\n",
    "    StructField(\"transaction_id\", LongType(), True),\n",
    "    StructField(\"transaction_start_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "\n",
    "#2159\n",
    "data = [\n",
    "    (100.0, 101, 2001, date(2023, 4, 10)),   # April\n",
    "    (250.0, 102, 2002, date(2023, 5, 15)),   # May\n",
    "    (300.0, 103, 2003, date(2023, 6, 12)),   # June\n",
    "    (150.0, 101, 2004, date(2023, 5, 22)),   # May again for 101\n",
    "    (175.0, 104, 2005, date(2023, 3, 28)),   # March\n",
    "    (220.0, 105, 2006, date(2023, 4, 2)),    # April\n",
    "    (400.0, 106, 2007, date(2023, 1, 30))    # January\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "tx_df = spark.createDataFrame(data, schema)\n",
    "tx_df.createOrReplaceTempView(\"from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, month\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"TransactionFilter\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"amt\", DoubleType(), True),\n",
    "    StructField(\"signup_id\", LongType(), True),\n",
    "    StructField(\"transaction_id\", LongType(), True),\n",
    "    StructField(\"transaction_start_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (100.0, 101, 2001, date(2023, 4, 10)),   # April\n",
    "    (250.0, 102, 2002, date(2023, 5, 15)),   # May\n",
    "    (300.0, 103, 2003, date(2023, 6, 12)),   # June\n",
    "    (150.0, 101, 2004, date(2023, 5, 22)),   # May again for 101\n",
    "    (175.0, 104, 2005, date(2023, 3, 28)),   # March\n",
    "    (220.0, 105, 2006, date(2023, 4, 2)),    # April\n",
    "    (400.0, 106, 2007, date(2023, 1, 30))    # January\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "tx_df = spark.createDataFrame(data, schema)\n",
    "tx_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# Show the table\n",
    "tx_df.show()\n",
    "\")\n",
    "\n",
    "# Show the table\n",
    "tx_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744a995c-8bfb-400f-bc76-7aaa865ef8d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amt: double (nullable = true)\n",
      " |-- signup_id: long (nullable = true)\n",
      " |-- transaction_id: long (nullable = true)\n",
      " |-- transaction_start_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tx_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c17fa07-d832-4822-8104-91ebbd7354aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|signup_id|\n",
      "+---------+\n",
      "|      101|\n",
      "|      102|\n",
      "|      105|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        select DISTINCT(signup_id) from transactions\n",
    "        where MONTH(transaction_start_date) IN (4,5)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114eddfd-c084-4bec-9c7b-f930a205ac8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Undefined function: MONTHNAME. This function is neither a built-in/temporary function, nor a persistent function that is qualified as spark_catalog.default.monthname.; line 2 pos 39",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98/3530474997.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m         \u001b[0mselect\u001b[0m \u001b[0mtransaction_start_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMONTHNAME\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransaction_start_date\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m      5\u001b[0m ).show()\n",
      "\u001b[0;32m/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Undefined function: MONTHNAME. This function is neither a built-in/temporary function, nor a persistent function that is qualified as spark_catalog.default.monthname.; line 2 pos 39"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        select transaction_start_date, MONTHNAME(transaction_start_date) from transactions\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954b399-27d8-44ec-bd2d-102ed2864091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

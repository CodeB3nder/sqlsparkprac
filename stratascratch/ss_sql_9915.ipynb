{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8a868a-61f8-4243-a663-3a1e25c1e34e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "spark = SparkSession.builder.appName(\"PySparkTables\").getOrCreate()\n",
    "# customers data\n",
    "customers_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Carol\")\n",
    "]\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), False)\n",
    "])\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data, customers_schema)\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "# orders data\n",
    "orders_data = [\n",
    "    (101, 1, \"2019-02-05\", 50.00),\n",
    "    (102, 1, \"2019-02-05\", 70.00),\n",
    "    (103, 2, \"2019-02-05\", 80.00),\n",
    "    (104, 1, \"2019-03-10\", 120.00),\n",
    "    (105, 3, \"2019-04-20\", 200.00),\n",
    "    (106, 2, \"2019-04-20\", 150.00)\n",
    "]\n",
    "\n",
    "orders_data_typed = [\n",
    "    (order_id, cust_id, datetime.strptime(order_date, \"%Y-%m-%d\").date(), total_cost)\n",
    "    for order_id, cust_id, order_date, total_cost in orders_data\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"cust_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", DateType(), False),\n",
    "    StructField(\"total_order_cost\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data_typed, orders_schema)\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aae15ff1-0eb7-4769-a095-71f1ceec2e38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|first_name|total_per_day_cost|\n",
      "+----------+------------------+\n",
      "|     Alice|             120.0|\n",
      "|     Alice|             120.0|\n",
      "|     Carol|             200.0|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    With cte as (\n",
    "    select cust_id, order_date, SUM(total_order_cost) as total_per_day_cost \n",
    "    from orders group by cust_id, order_date\n",
    "    having order_date between '2019-02-01' AND '2019-05-01'),\n",
    "    cte2 as (\n",
    "    select *,\n",
    "    rank() over(partition by order_date order by total_per_day_cost desc) as rnk\n",
    "    from cte)\n",
    "    select c.first_name, c2.total_per_day_cost  from cte2 c2 join customers c on c.id = c2.cust_id where rnk = 1 order by order_date\n",
    "    \n",
    "    \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c353cf-4350-4bad-8dc3-c0b304d6d731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

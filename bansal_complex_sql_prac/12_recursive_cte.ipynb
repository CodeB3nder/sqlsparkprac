{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63cb5a02-5805-47a3-be0d-f323be01a8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales table and view created successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SalesData\").getOrCreate()\n",
    "\n",
    "# Define schema for Sales table\n",
    "sales_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"period_start\", DateType(), True),\n",
    "    StructField(\"period_end\", DateType(), True),\n",
    "    StructField(\"average_daily_sales\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Correctly parse date strings into datetime objects\n",
    "sales_data = [\n",
    "    (1, datetime.strptime(\"2019-01-25\", \"%Y-%m-%d\"), datetime.strptime(\"2019-02-28\", \"%Y-%m-%d\"), 100),\n",
    "    (2, datetime.strptime(\"2018-12-01\", \"%Y-%m-%d\"), datetime.strptime(\"2020-01-01\", \"%Y-%m-%d\"), 10),\n",
    "    (3, datetime.strptime(\"2019-12-01\", \"%Y-%m-%d\"), datetime.strptime(\"2020-01-31\", \"%Y-%m-%d\"), 1)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, schema=sales_schema)\n",
    "\n",
    "# Create a temporary SQL view\n",
    "sales_df.createOrReplaceTempView(\"Sales\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Sales table and view created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c755ec-18ea-47fb-b88e-d0c00b65d916",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: r_cte; line 5 pos 45;\n'WithCTE\n:- 'CTERelationDef 1, false\n:  +- 'SubqueryAlias r_cte\n:     +- 'Union false, false\n:        :- Aggregate [min(period_start#1) AS dates#12, max(period_end#2) AS max_date#13]\n:        :  +- SubqueryAlias sales\n:        :     +- View (`Sales`, [product_id#0,period_start#1,period_end#2,average_daily_sales#3])\n:        :        +- LogicalRDD [product_id#0, period_start#1, period_end#2, average_daily_sales#3], false\n:        +- 'Project [unresolvedalias('DATE_ADD('dates, 1), None), 'max_date]\n:           +- 'Filter ('dates < 'max_date)\n:              +- 'UnresolvedRelation [r_cte], [], false\n+- 'Project [*]\n   +- 'SubqueryAlias r_cte\n      +- 'CTERelationRef 1, false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2665/2376998096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mSELECT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mFROM\u001b[0m \u001b[0mr_cte\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \"\"\")\n\u001b[0m",
      "\u001b[0;32m/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: r_cte; line 5 pos 45;\n'WithCTE\n:- 'CTERelationDef 1, false\n:  +- 'SubqueryAlias r_cte\n:     +- 'Union false, false\n:        :- Aggregate [min(period_start#1) AS dates#12, max(period_end#2) AS max_date#13]\n:        :  +- SubqueryAlias sales\n:        :     +- View (`Sales`, [product_id#0,period_start#1,period_end#2,average_daily_sales#3])\n:        :        +- LogicalRDD [product_id#0, period_start#1, period_end#2, average_daily_sales#3], false\n:        +- 'Project [unresolvedalias('DATE_ADD('dates, 1), None), 'max_date]\n:           +- 'Filter ('dates < 'max_date)\n:              +- 'UnresolvedRelation [r_cte], [], false\n+- 'Project [*]\n   +- 'SubqueryAlias r_cte\n      +- 'CTERelationRef 1, false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH r_cte AS (\n",
    "    SELECT MIN(period_start) AS dates, MAX(period_end) AS max_date FROM Sales\n",
    "    UNION ALL\n",
    "    SELECT DATE_ADD(dates, 1), max_date FROM r_cte WHERE dates < max_date\n",
    "    )\n",
    "\n",
    "    SELECT * FROM r_cte;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4564b32-f9d7-43c0-ac47-eefb93968b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_date, max_date = sales_df.select(min(\"period_start\"), max(\"period_end\")).first()\n",
    "\n",
    "# Initialize result list with start date\n",
    "date_list = [(min_date, max_date)]\n",
    "\n",
    "# Generate dates iteratively\n",
    "current_date = min_date\n",
    "while current_date < max_date:\n",
    "    current_date = current_date + timedelta(days=1)\n",
    "    date_list.append((current_date, max_date))\n",
    "\n",
    "# Create DataFrame from the date list\n",
    "date_df = spark.createDataFrame(date_list, [\"dates\", \"max_date\"])\n",
    "\n",
    "# Show the results\n",
    "result_df = date_df.alias(\"d\").join(\n",
    "    sales_df.alias(\"s\"), \n",
    "    (col(\"d.dates\").between(col(\"s.period_start\"), col(\"s.period_end\"))),  # Correct usage of between\n",
    "    \"inner\"\n",
    ").orderBy(col(\"product_id\"), col(\"dates\"))\n",
    "\n",
    "result_df = result_df.groupBy(\n",
    "    col(\"product_id\"), year(col(\"dates\"))\n",
    ").agg(\n",
    "    sum(col(\"average_daily_sales\")).alias(\"TA\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4ba66f7-9726-400d-8053-74fcf18cad84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----+\n",
      "|product_id|year(dates)|  TA|\n",
      "+----------+-----------+----+\n",
      "|         2|       2018| 310|\n",
      "|         2|       2019|3650|\n",
      "|         1|       2019|3500|\n",
      "|         3|       2019|  31|\n",
      "|         2|       2020|  10|\n",
      "|         3|       2020|  31|\n",
      "+----------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47a382-ea0e-44db-98a3-0b19400420a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
